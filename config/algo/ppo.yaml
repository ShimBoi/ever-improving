# defaults:
#- policy: ppo
#- optim: base

name: "ppo"

policy_kwargs:
  share_features_extractor: True

gamma: 0.99
gae_lambda: 0.95
vf_coef: 0.5
ent_coef: 0.0
target_kl: null

learning_rate: 3e-4
batch_size: 256
n_steps: 2048

stats_window_size: 10 # was 100 ... for logging only
seed: 0


# use_original_space: False
# warmup_zero_action: False
